\documentclass{amsart}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thmtools,thm-restate}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[singlelinecheck=false]{caption}
\usepackage[backend=biber,url=true,doi=true,eprint=false,style=alphabetic]{biblatex}
\usepackage{enumitem}
\usepackage[justification=centering]{caption}
\usepackage{indentfirst}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage[x11names, rgb]{xcolor}
\usepackage{tikz}
\usepackage{hyperref}
\usetikzlibrary{snakes,arrows,shapes}

\addbibresource{references.bib}

\makeatletter
\def\subsection{\@startsection{subsection}{3}%
  \z@{.5\linespacing\@plus.7\linespacing}{.1\linespacing}%
  {\normalfont}}
\makeatother

\makeatletter
\patchcmd{\@setauthors}{\MakeUppercase}{}{}{}
\makeatother

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Val}{\text{Val}}
\DeclareMathOperator*{\Ch}{\text{Ch}}
\DeclareMathOperator*{\Pa}{\text{Pa}}
\DeclareMathOperator*{\Sc}{\text{Sc}}
\newcommand{\ov}{\overline}

\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\algrenewcommand\algorithmicrequire{\textbf{Input}}
\algrenewcommand\algorithmicensure{\textbf{Output}}

\captionsetup[table]{labelsep=space}

\theoremstyle{plain}

\newcounter{dummy-def}\numberwithin{dummy-def}{section}
\newtheorem{definition}[dummy-def]{Definition}
\newcounter{dummy-thm}\numberwithin{dummy-thm}{section}
\newtheorem{theorem}[dummy-thm]{Theorem}
\newcounter{dummy-prop}\numberwithin{dummy-prop}{section}
\newtheorem{proposition}[dummy-prop]{Proposition}
\newcounter{dummy-corollary}\numberwithin{dummy-corollary}{section}
\newtheorem{corollary}[dummy-corollary]{Corollary}
\newcounter{dummy-ex}\numberwithin{dummy-ex}{section}
\newtheorem{exercise}[dummy-ex]{Exercise}
\newcounter{dummy-eg}\numberwithin{dummy-eg}{section}
\newtheorem{example}[dummy-eg]{Example}

\numberwithin{equation}{section}

\newcommand{\set}[1]{\mathbf{#1}}
\newcommand{\pr}{\mathbb{P}}
\renewcommand{\implies}{\Rightarrow}

\newcommand{\bigo}{\mathcal{O}}

\setlength{\parskip}{1em}

\lstset{frameround=fttt,
	numbers=left,
	breaklines=true,
	keywordstyle=\bfseries,
	basicstyle=\ttfamily,
}

\newcommand{\code}[1]{\lstinline[mathescape=true]{#1}}
\newcommand{\mcode}[1]{\lstinline[mathescape]!#1!}


\title{%
  \noindent\rule{13cm}{1.0pt}\\
  \vspace{0.2cm}
  Analysis on an Implementation of the Gens-Domingos Sum-Product Network Structural Learning
  Schema
  \noindent\rule{13cm}{0.8pt}
}
\xdef\shorttitle{Analysis on the GD Schema}
\author[]{\normalsize\textbf{Renato Lui Geh}\\\small Computer Science\\Institute of Mathematics
  and Statistics\\University of SÃ£o Paulo\\\texttt{renatolg@ime.usp.br}}

\begin{document}

\begin{abstract}
  Sum-Product Networks (SPNs) are a class of deep probabilistic graphical models. Inference in them
  is linear in the number of edges of the graph. Furthermore, exact inference is achieved, in a
  valid SPN, by running through its edges twice at most, making exact inference linear. The
  Gens-Domingos SPN Schema is an algorithm for structural learning on such models. In this paper we
  present an implementation of such schema, analyzing its complexity, discoursing implementational
  and theoretical details, and finally presenting results and experiments achieved with this
  implementation.

  \smallskip
  \smallskip
  \smallskip
  \textbf{Keywords}
  \smallskip
  \texttt{cluster analysis; data mining; probabilistic graphical models; tractable models; machine
  learning; deep learning}
  \vspace*{-3.5em}
\end{abstract}

\maketitle

\section{Introduction}

A Sum-Product Network (SPN) is a probabilistic graphical model that represents a tractable
distribution of probability. If an SPN is valid, then we can perform exact inference in time linear
to the graph's edges. Its syntax is different to other conventional models (read bayesian and
markov networks) in the sense that its graph does not explicitly model events and (in) dependencies
between variables. That is, whilst variables in a bayesian network are represented as nodes in the
graph, with each edge connecting two nodes asserting a dependency relationship between the
connected variables, a node in an SPN may not necessarily represent a variable or event, neither an
edge connecting two nodes represent dependence. In this sense, SPNs can be seen as a type of
probabilistic Artificial Neural Network (ANN). However, whilst neural networks represent a
function, SPNs model a tractable probability distribution. Furthermore, SPNs are distinct from
standard neural networks seeing that, whereas ANNs have only one type of neuron with an activation
function mapping to values in $[0,1]$, SPNs have two kind of neurons, which we will see in the next
sections. Still, SPNs retain certain important characteristics from ANNs as we will discuss later,
with mainly its deep structure properties as the most interesting feature.

The Gens-Domingos Schema~\cite{gens-domingos}, or \code{LearnGD} as we will reference it throughout
this paper, is an SPN structural learning algorithm proposed by Robert Gens and Pedro Domingos.
Gens and Domingos call it a schema because it only provides a template of what the algorithm should
be like. We will discuss \code{LearnGD} in details in the next section. This paper documents a
particular implementation of the GD schema. Other implementations may have different results.

In this document, we show how we implemented the \code{LearnGD} algorithm. We analyse the
complexity of each algorithm component in detail, later referring to such analyses when drawing
conclusions on the overall complexity of the algorithm. As we have mentioned before, since the
\code{LearnGD} schema depends heavily on implementation, the complexity we achieve in this
particular case may differ from other implementations. After each analysis, we then look at the
algorithm as whole, drawing conclusions on time and memory usage, as well as implementation details
that could potentially decrease the algorithm runtime. We also comment on how to implement better
concurrency then how it is currently coded in our implementation. We then show some results on
experiments made on image classification and image completion.

\section{Sum-Product Networks}

In this section we will define SPNs differently from other articles~\cite{gens-domingos,
poon-domingos, clustering} as the original more convoluted definition is of little use for the
\code{LearnGD} algorithm. Our definition is almost identical to the original \code{LearnGD} article
\cite{gens-domingos}, with the exception that we assume that an SPN is already normalized. This
fact changes nothing, since Peharz \textit{et al} recently proved that normalized SPNs have as much
representability power as unnormalized SPNs~\cite{theoretical-spn}. Before we enunciate the formal
definition of an SPN, we will give an informal, vague definition of an SPN in order to explain what
completeness, consistency, validity and decomposability --- which are an important set of
definitions --- of an SPN mean.

A sum-product network represents a tractable probability distribution through a DAG\@. Such digraph
must always be weakly connected. A node can either be a leaf, a sum, or a product node. The scope
of a node is the set of all variables present in all its descendants. Leaf nodes are tractable
probability distributions and their scope is the scope of its distribution, sum nodes represent the
summing out of the variables in its scope and product nodes act as feature hierarchy. An edge that
has its origin from a sum node has a non-negative weight. We refer to a sub-SPN $S$ rooted at node
$i$ as $S(i)$, while the SPN rooted at its root is denoted as $S(\cdot)$ or simply $S$. The scope
of a node will be denoted as $\Sc(i)$, where $i$ is a node. The set of children of a node will be
denoted as $\Ch(i)$. Similarly, $\Pa(i)$ is the set of parents of node $i$.

\begin{definition}[Normalized]~\\
  Let $S$ be an SPN and $\Sigma(S)$ be the set of all sum nodes of $S$. $S$ is normalized iff, for
  all $\sigma \in \Sigma(S)$, $\sum_{c\in Ch(\sigma)} w_{\sigma c} = 1$ and $0 \leq w_{\sigma c}
  \leq 1$, where $w_{\sigma c}$ is the weight from edge $\sigma \to c$.
\end{definition}

\begin{definition}[Completeness]~\\
  Let $S$ be an SPN and $\Sigma(S)$ be the set of all sum nodes of $S$. $S$ is complete iff, for
  all $\sigma \in \Sigma(S)$, $Sc(i)=Sc(j), i\neq j; \forall i,j\in \Ch(\sigma)$.
\end{definition}

\begin{definition}[Consistency]~\\
  Let $S$ be an SPN, $\Pi(S)$ be the set of all product nodes of $S$ and $X$ a variable in $Sc(S)$.
  $S$ is consistent iff $X$ takes the same value for all elements in $\Pi(S)$ that contain $X$.
\end{definition}

\begin{definition}[Validity]~\\
  An SPN $S$ is valid iff it always computes the correct probability of evidence $S$ represents.
\end{definition}

\begin{theorem}\label{thm:validity}
  An SPN $S$ is valid if it is both complete and consistent.
\end{theorem}

Validity guarantees that the SPN will compute not only the correct probability of evidence, but
also in time linear to its graph's edges. Therefore, it is preferable to learn valid SPNs. Notice
that~\autoref{thm:validity} is not restricted by completeness and consistency. In fact, incomplete
and/or inconsistent SPNs can compute the probability of evidence correctly, but consistency and
completeness guarantee that all sub-SPNs are also valid.

\begin{definition}[Decomposability]~\\
  Let $S$ be an SPN and $\Pi(S)$ be the set of all product nodes in $S$. $S$ is decomposable iff,
  for all $\pi \in \Pi(S)$, $Sc(i)\cap Sc(j)=\emptyset, i\neq j; \forall i,j\in \Ch(\pi)$.
\end{definition}

It is clear that decomposability implies consistency, therefore if an SPN is both complete and
decomposable, than it is also valid. We choose to work with decomposability because it is easier to
learn decomposable SPNs then it is to learn consistent ones. We do not lose representation power
because a complete and consistent SPN can be transformed into a complete and decomposable SPN in no
more than a polynomial number of edge and node additions~\cite{theoretical-spn}. We can now
formally define an SPN\@.

\begin{definition}[Sum-product network]~\\
  A sum-product network (SPN) is a weakly connected DAG that can be recursively defined as
  following.

  An SPN\@:
  \begin{enumerate}
    \item with a single node is a monovariate tractable probability distribution (\textbf{leaf});
    \item is a normalized weighted sum of SPNs (\textbf{sum});
    \item is a product of SPNs (\textbf{product}).
  \end{enumerate}
  The value of an SPN is defined by its type. Let $\lambda$, $\sigma$ and $\pi$ be a leaf, sum and
  product respectively. The values of such SPNs are given by $\lambda(\mathbf{x})$,
  $\sigma(\mathbf{x})$ and $\pi(\mathbf{x})$, where $\mathbf{x}$ is a certain evidence
  instantiation.
  \begin{description}
    \item[Leaf] $\lambda(\mathbf{x})$ is the value of the probability distribution at point
      $\mathbf{x}$.
    \item[Product] $\pi(\mathbf{x}) = \prod_{c \in \Ch(\pi)} c(\mathbf{x})$.
    \item[Sum] $\sigma(\mathbf{x}) = \sum_{c \in \Ch(\sigma)} w_{\sigma c} c(\mathbf{x})$, with
      $\sum_{c \in \Ch(\sigma)} w_{\sigma c} = 1$ and $0 \leq w_{\sigma c} \leq 1$.
  \end{description}
\end{definition}

Note that this definition assumes an SPN to be complete, decomposable and normalized. Other
definitions in literature may differ from ours, but as we have mentioned before, for our
implementation, this definition is convenient for us. Another observation worthy of notice is the
value of $\lambda(\mathbf{x})$. Although here we consider $\mathbf{x}$ to be a multivariate
instantiation (i.e.\ a set of --- potentially multiple --- variable valuations), we had initially
defined a leaf to be a monovariate distribution. Although it is possible to attribute leaves as
multivariate probability distributions~\cite{id-spn}, for our definition we have chosen to keep a
leaf's scope unitary. Therefore, in the case of a leaf's value, $\mathbf{x}$ is a unitary
(monovariate) variable instantiation.

\section{The \code{LearnGD} Schema}

The \code{LearnGD} schema was proposed by Robert Gens and Pedro Domingos on \textit{Learning the
Structure of Sum-Product Networks}~\cite{gens-domingos}. In this section we will outline the schema
in pseudo-code and analyse a few properties derived from the algorithm.



%--------------------------------------------------------------------------------------------------

\newpage
\appendix

\newpage

\printbibliography[]

\end{document}
