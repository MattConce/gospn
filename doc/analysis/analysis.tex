\documentclass{amsart}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thmtools,thm-restate}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[singlelinecheck=false]{caption}
\usepackage[backend=biber,url=true,doi=true,eprint=false,style=alphabetic]{biblatex}
\usepackage{enumitem}
\usepackage[justification=centering]{caption}
\usepackage{indentfirst}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage[x11names, rgb]{xcolor}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{subcaption}
\usetikzlibrary{snakes,arrows,shapes}

\addbibresource{references.bib}

\makeatletter
\def\subsection{\@startsection{subsection}{3}%
  \z@{.5\linespacing\@plus.7\linespacing}{.1\linespacing}%
  {\normalfont}}
\makeatother

\makeatletter
\patchcmd{\@setauthors}{\MakeUppercase}{}{}{}
\makeatother

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Val}{\text{Val}}
\DeclareMathOperator*{\Ch}{\text{Ch}}
\DeclareMathOperator*{\Pa}{\text{Pa}}
\DeclareMathOperator*{\Sc}{\text{Sc}}
\newcommand{\ov}{\overline}

\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\newcommand{\algorithmautorefname}{Algorithm}
\algrenewcommand\algorithmicrequire{\textbf{Input}}
\algrenewcommand\algorithmicensure{\textbf{Output}}

\captionsetup[table]{labelsep=space}

\theoremstyle{plain}

\newcounter{dummy-def}\numberwithin{dummy-def}{section}
\newtheorem{definition}[dummy-def]{Definition}
\newcounter{dummy-thm}\numberwithin{dummy-thm}{section}
\newtheorem{theorem}[dummy-thm]{Theorem}
\newcounter{dummy-prop}\numberwithin{dummy-prop}{section}
\newtheorem{proposition}[dummy-prop]{Proposition}
\newcounter{dummy-corollary}\numberwithin{dummy-corollary}{section}
\newtheorem{corollary}[dummy-corollary]{Corollary}
\newcounter{dummy-lemma}\numberwithin{dummy-lemma}{section}
\newtheorem{lemma}[dummy-lemma]{Lemma}
\newcounter{dummy-ex}\numberwithin{dummy-ex}{section}
\newtheorem{exercise}[dummy-ex]{Exercise}
\newcounter{dummy-eg}\numberwithin{dummy-eg}{section}
\newtheorem{example}[dummy-eg]{Example}

\numberwithin{equation}{section}

\newcommand{\set}[1]{\mathbf{#1}}
\newcommand{\pr}{\mathbb{P}}
\renewcommand{\implies}{\Rightarrow}

\newcommand{\bigo}{\mathcal{O}}

\setlength{\parskip}{1em}

\lstset{frameround=fttt,
	numbers=left,
	breaklines=true,
	keywordstyle=\bfseries,
	basicstyle=\ttfamily,
}

\newcommand{\code}[1]{\lstinline[mathescape=true]{#1}}
\newcommand{\mcode}[1]{\lstinline[mathescape]!#1!}


\title{%
  \noindent\rule{13cm}{1.0pt}\\
  \vspace{0.2cm}
  Analysis on an Implementation of the Gens-Domingos Sum-Product Network Structural Learning
  Schema
  \noindent\rule{13cm}{0.8pt}
}
\xdef\shorttitle{Analysis on the GD Schema}
\author[]{\normalsize\textbf{Renato Lui Geh}\\\small Computer Science\\Institute of Mathematics
  and Statistics\\University of SÃ£o Paulo\\\texttt{renatolg@ime.usp.br}}

\begin{document}

\begin{abstract}
  Sum-Product Networks (SPNs) are a class of deep probabilistic graphical models. Inference in them
  is linear in the number of edges of the graph. Furthermore, exact inference is achieved, in a
  valid SPN, by running through its edges twice at most, making exact inference linear. The
  Gens-Domingos SPN Schema is an algorithm for structural learning on such models. In this paper we
  present an implementation of such schema, analyzing its complexity, discoursing implementational
  and theoretical details, and finally presenting results and experiments achieved with this
  implementation.

  \smallskip
  \smallskip
  \smallskip
  \textbf{Keywords}
  \smallskip
  \texttt{cluster analysis; data mining; probabilistic graphical models; tractable models; machine
  learning; deep learning}
  \vspace*{-3.5em}
\end{abstract}

\maketitle

\section{Introduction}

A Sum-Product Network (SPN) is a probabilistic graphical model that represents a tractable
distribution of probability. If an SPN is valid, then we can perform exact inference in time linear
to the graph's edges. Its syntax is different to other conventional models (read bayesian and
markov networks) in the sense that its graph does not explicitly model events and (in) dependencies
between variables. That is, whilst variables in a bayesian network are represented as nodes in the
graph, with each edge connecting two nodes asserting a dependency relationship between the
connected variables, a node in an SPN may not necessarily represent a variable or event, neither an
edge connecting two nodes represent dependence. In this sense, SPNs can be seen as a type of
probabilistic Artificial Neural Network (ANN). However, whilst neural networks represent a
function, SPNs model a tractable probability distribution. Furthermore, SPNs are distinct from
standard neural networks seeing that, whereas ANNs have only one type of neuron with an activation
function mapping to values in $[0,1]$, SPNs have two kind of neurons, which we will see in the next
sections. Still, SPNs retain certain important characteristics from ANNs as we will discuss later,
with mainly its deep structure properties~\cite{shallow-vs-deep} as the most interesting feature.

The Gens-Domingos Schema~\cite{gens-domingos}, or \code{LearnGD} as we will reference it throughout
this paper, is an SPN structural learning algorithm proposed by Robert Gens and Pedro Domingos.
Gens and Domingos call it a schema because it only provides a template of what the algorithm should
be like. We will discuss \code{LearnGD} in details in the next section. This paper documents a
particular implementation of the GD schema. Other implementations may have different results.

In this document, we show how we implemented the \code{LearnGD} algorithm. We analyze the
complexity of each algorithm component in detail, later referring to such analyzes when drawing
conclusions on the overall complexity of the algorithm. As we have mentioned before, since the
\code{LearnGD} schema depends heavily on implementation, the complexity we achieve in this
particular case may differ from other implementations. After each analysis, we then look at the
algorithm as whole, drawing conclusions on time and memory usage, as well as implementation details
that could potentially decrease the algorithm runtime. We also comment on how to implement better
concurrency then how it is currently coded in our implementation. We then show some results on
experiments made on image classification and image completion.

\section{Sum-Product Networks}

In this section we will define SPNs differently from other articles~\cite{gens-domingos,
poon-domingos, clustering} as the original more convoluted definition is of little use for the
\code{LearnGD} algorithm. Our definition is almost identical to the original \code{LearnGD} article
\cite{gens-domingos}, with the exception that we assume that an SPN is already normalized. This
fact changes nothing, since Peharz \textit{et al} recently proved that normalized SPNs have as much
representability power as unnormalized SPNs~\cite{theoretical-spn}. Before we enunciate the formal
definition of an SPN, we will give an informal, vague definition of an SPN in order to explain what
completeness, consistency, validity and decomposability --- which are an important set of
definitions --- of an SPN mean.

A sum-product network represents a tractable probability distribution through a DAG\@. Such digraph
must always be weakly connected. A node can either be a leaf, a sum, or a product node. The scope
of a node is the set of all variables present in all its descendants. Leaf nodes are tractable
probability distributions and their scope is the scope of its distribution, sum nodes represent the
summing out of the variables in its scope and product nodes act as feature hierarchy. An edge that
has its origin from a sum node has a non-negative weight. We refer to a sub-SPN $S$ rooted at node
$i$ as $S(i)$, while the SPN rooted at its root is denoted as $S(\cdot)$ or simply $S$. The scope
of a node will be denoted as $\Sc(i)$, where $i$ is a node. The set of children of a node will be
denoted as $\Ch(i)$. Similarly, $\Pa(i)$ is the set of parents of node $i$.

\begin{definition}[Normalized]~\\
  Let $S$ be an SPN and $\Sigma(S)$ be the set of all sum nodes of $S$. $S$ is normalized iff, for
  all $\sigma \in \Sigma(S)$, $\sum_{c\in Ch(\sigma)} w_{\sigma c} = 1$ and $0 \leq w_{\sigma c}
  \leq 1$, where $w_{\sigma c}$ is the weight from edge $\sigma \to c$.
\end{definition}

\begin{definition}[Completeness]~\\
  Let $S$ be an SPN and $\Sigma(S)$ be the set of all sum nodes of $S$. $S$ is complete iff, for
  all $\sigma \in \Sigma(S)$, $\Sc(i)=\Sc(j), i\neq j; \forall i,j\in \Ch(\sigma)$.
\end{definition}

\begin{definition}[Consistency]~\\
  Let $S$ be an SPN, $\Pi(S)$ be the set of all product nodes of $S$ and $X$ a variable in
  $\Sc(S)$. $S$ is consistent iff $X$ takes the same value for all elements in $\Pi(S)$ that
  contain $X$.
\end{definition}

\begin{definition}[Validity]~\\
  An SPN $S$ is valid iff it always computes the correct probability of evidence $S$ represents.
\end{definition}

\begin{theorem}\label{thm:validity}
  An SPN $S$ is valid if it is both complete and consistent.
\end{theorem}

Validity guarantees that the SPN will compute not only the correct probability of evidence, but
also in time linear to its graph's edges. Therefore, it is preferable to learn valid SPNs. Notice
that~\autoref{thm:validity} is not restricted by completeness and consistency. In fact, incomplete
and/or inconsistent SPNs can compute the probability of evidence correctly, but consistency and
completeness guarantee that all sub-SPNs are also valid.

\begin{definition}[Decomposability]~\\
  Let $S$ be an SPN and $\Pi(S)$ be the set of all product nodes in $S$. $S$ is decomposable iff,
  for all $\pi \in \Pi(S)$, $\Sc(i)\cap \Sc(j)=\emptyset, i\neq j; \forall i,j\in \Ch(\pi)$.
\end{definition}

It is clear that decomposability implies consistency, therefore if an SPN is both complete and
decomposable, than it is also valid. We choose to work with decomposability because it is easier to
learn decomposable SPNs then it is to learn consistent ones. We do not lose representation power
because a complete and consistent SPN can be transformed into a complete and decomposable SPN in no
more than a polynomial number of edge and node additions~\cite{theoretical-spn}. We can now
formally define an SPN\@.

\begin{definition}[Sum-product network]~\\
  A sum-product network (SPN) is a weakly connected DAG that can be recursively defined as
  following.

  An SPN\@:
  \begin{enumerate}
    \item with a single node is a univariate tractable probability distribution (\textbf{leaf});
    \item is a normalized weighted sum of SPNs of same scope (\textbf{sum});
    \item is a product of SPNs with disjoint scopes (\textbf{product}).
  \end{enumerate}
  The value of an SPN is defined by its type. Let $\lambda$, $\sigma$ and $\pi$ be a leaf, sum and
  product respectively. The values of such SPNs are given by $\lambda(\mathbf{x})$,
  $\sigma(\mathbf{x})$ and $\pi(\mathbf{x})$, where $\mathbf{x}$ is a certain evidence
  instantiation.
  \begin{description}
    \item[Leaf] $\lambda(\mathbf{x})$ is the value of the probability distribution at point
      $\mathbf{x}$.
    \item[Product] $\pi(\mathbf{x}) = \prod_{c \in \Ch(\pi)} c(\mathbf{x})$.
    \item[Sum] $\sigma(\mathbf{x}) = \sum_{c \in \Ch(\sigma)} w_{\sigma c} c(\mathbf{x})$, with
      $\sum_{c \in \Ch(\sigma)} w_{\sigma c} = 1$ and $0 \leq w_{\sigma c} \leq 1$.
  \end{description}
\end{definition}

Note that this definition assumes an SPN to be complete, decomposable and normalized. Other
definitions in literature may differ from ours, but as we have mentioned before, for our
implementation, this definition is convenient for us. Another observation worthy of notice is the
value of $\lambda(\mathbf{x})$. Although here we consider $\mathbf{x}$ to be a multivariate
instantiation (i.e.\ a set of --- potentially multiple --- variable valuations), we had initially
defined a leaf to be a univariate distribution. Although it is possible to attribute leaves as
multivariate probability distributions~\cite{id-spn}, for our definition we have chosen to keep a
leaf's scope a unit set. Therefore, in the case of a leaf's value, $\mathbf{x}$ is a singleton
(univariate) variable instantiation.

\section{The \code{LearnGD} Schema}

The \code{LearnGD} schema was proposed by Robert Gens and Pedro Domingos on \textit{Learning the
Structure of Sum-Product Networks}~\cite{gens-domingos}. In this section we will outline the schema
in pseudo-code and analyze a few properties derived from the algorithm.

\begin{algorithm}[H]
  \caption{\code{LearnGD}}\label{alg:learngd}
  \begin{algorithmic}[1]
    \Require\,Set $\mathbf{D}$ of instances (data)
    \Require\,Set $\mathbf{V}$ of variables (scope)
    \Ensure\,An SPN representing a probability distribution given by $\mathbf{D}$ and $\mathbf{V}$
    \If{$|\set{V}|=1$} \Comment{univariate data sample}
      \State\,\textbf{return} univariate distribution estimated from $T[\set{V}]$ (data of
        $\set{V}$)
    \EndIf%
    \State\,Take $\set{V}$ and find mutually independent subsets $\set{V}_i$ of variables
    \If{possible to partition} \Comment{i.e.\ we have found independent subsets}
      \State\,\textbf{return} $\prod_i$ \mcode{LearnGD ($\set{D}$, $\set{V}_i$)}
    \Else\Comment{we cannot say there is independence}
      \State\,Take $\set{D}$ and find $\set{D}_j$ subsets of similar instances
      \If{possible to partition}
        \State\,\textbf{return} $\sum_i \frac{|\set{D}_j|}{|\set{D}|} \cdot$ \mcode{LearnGD
        ($\set{D}_j$, $\set{V}$)}
      \Else\Comment{i.e.\ data is one big cluster}
        \State\,\textbf{return} fully factorized distribution.
      \EndIf%
    \EndIf%
  \end{algorithmic}
\end{algorithm}

Let us now, for a moment, suppose that SPNs are not necessarily complete, decomposable and
normalized. We shall prove a few results derived from SPNs generated by~\autoref{alg:learngd}.

\begin{lemma}
  An SPN $S$ generated by \code{LearnGD} is complete, decomposable and normalized.
\end{lemma}
\begin{proof}
  Lines 4--6 show that the scope of each child in a product node of $S$ is a partition of the scope
  of their parent. Therefore, children have pairwise disjoint scopes on line 6, which proves
  decomposability for this part of the algorithm. In lines 8--10, since we are clustering similar
  instances, $\set{D}$ is being partitioned but we are not changing $\set{V}$ in any way. In fact,
  line 10 shows that we pass $\set{V}$ to all other children. That is, all children of sum nodes
  have the same scope as their parent, which proves completeness.  Let $\set{D}_1,\ldots,\set{D}_n$
  be the subsets of similar instances. By the definition of clustering, $\set{D}_1\cup\ldots\cup
  \set{D}_n=\set{D}$ and $\set{D}_i\cap \set{D}_j= \emptyset$, $i\neq j$, $1\leq i,j\leq n$. Thus
  it follows that $\sum_{i=1}^n \frac{|\set{D}_i|}{|\set{D}|}=1$ and thus line 10 always creates
  complete and normalized sum nodes. Line 12 is a special case where, if we have discovered that
  $\set{D}$ is one big data cluster, we shall create a product node $\pi$ in which all children of
  $\pi$ are leaves and \begin{equation*} \bigcup_{\lambda\in\Ch(\pi)} \Sc(\lambda) = \Sc(\pi).
  \end{equation*} In other words, we fully factorize our product node into leaves. In this case, it
  is obvious that this product node is decomposable.
\end{proof}

\code{LearnGD} can be divided into four parts:

\begin{enumerate}
  \item Is the data univariate? If it is, return a leaf.
  \item Are partitions of the data independent? If they are, return a product node whose children
    are the independent partitions.
  \item Are partitions of the data similar? If they are, return a sum node whose children are the
    partition clusters.
  \item In case all else fails, we have a fully factorized distribution.
\end{enumerate}

Going back to our definition of an SPN, we can now take a more intuitive approach and make the
following observations:

\begin{enumerate}
  \item A leaf is nothing but a local/partitioned/sample distribution of a probability distribution
    given by a single variable.
  \item A product node determines independence between variables.
  \item A sum node is a clustering of similar data values (i.e.\ instances that are ``alike'').
\end{enumerate}

This gives more semantic value to SPNs, whilst still retaining its expressivity. Following this
approach, one can easily notice that each ``layer'' corresponds to a recursive call in
\code{LearnGD}. In fact, each recursive call constructs a hidden layer that tries to partition the
SPN even further. This gives SPNs a deep architecture that resembles deep models in that the deeper
the model, the more representation power it has~\cite{shallow-vs-deep}.

Let us now observe the scope of each type of node. A leaf is the trivial case, since it has a
single variable in its scope by definition. Each layer above it can have either sum or product
nodes. Let us now look at decomposability, that is: if a variable $X$ appears in a child of a
product node $\pi$, then $X$ cannot appear in another child of $\pi$. This gives us the following
result:

\begin{lemma}\label{lemma-parents}
  Let $S$ be an SPN generated by \code{LearnGD}, and let $\Lambda(S)$ be the set of all leaves of
  $S$. Then, $\forall \lambda \in \Lambda(S)$, we have that, $\forall p \in \Pa(\lambda)$, $p$ is a
  product node.
\end{lemma}
\begin{proof}
  Our proof is by contradiction. Let us assume that $\exists p \in \Pa(\lambda)$ such that $p$ is
  a sum node and $\exists c^* \in \Ch(p)$ a leaf. From our assumption that $p$ is a sum node, we
  have that, since the SPN is complete, the scope of all children of $p$ are the same and are all
  equal to the scope of $p$. Now let $c \in \Ch(p)$. There must exist another child $c$ such that
  $c\neq c^*$ because of lines 5 and 9. From that we have $\Sc(c)=\Sc(c^*)$ because of
  completeness, and since $\Sc(c^*)$ is singular, then $c$ must also be leaf. But it is impossible
  to have leaves with same scope and same parent (line 1 from~\autoref{alg:learngd}). Therefore,
  $p$ is actually a product node.
\end{proof}

\begin{lemma}
  An SPN generated by \code{LearnGD} is a rooted tree.
\end{lemma}
\begin{proof}
  It suffices to show that for any vertex, its indegree is exactly one. We can prove that by saying
  that~\autoref{alg:learngd} never adds edges between two already existing vertices.
\end{proof}

\begin{definition}
  A sum-product network that is a rooted tree is called a sum-product tree (SPT).
\end{definition}

\begin{theorem}~\\
  Let $S$ be an SPT generated by \code{LearnGD}. Let $n=|\Sc(S)|$ and $m=|\set{D}|$, where
  $\set{D}$ is the data sent as parameter to \code{LearnGD}. Let $h$ be the height of $S$. Then
  \begin{equation*}
    1 \leq h \leq n + m - 1
  \end{equation*}
\end{theorem}
\begin{proof}
  Sketch of proof: every sum or product node creates one more hidden layer (increments SPT's height
  by one) and decrements either an instance (if sum node) or variable in node's scope (if product
  node) by one at least. Last (deepest) sum node must have at least two product nodes as children
  (following~\autoref{lemma-parents}), with each having one data instance each, therefore the
  number of layers created by sum nodes is at most $m-1$. Similarly for product nodes, if we want
  to maximize the number of layers, the deepest product node must have two leaves as children,
  bringing the total count of product nodes to $n-1$. Counting the last layer that is made out of
  leaves, we have $(n-1)+(m-1)+1=n+m-1$. Furthermore, the SPT has the form of alternated
  sum-product layers (a sum node will follow a product node and vice-versa). This guarantees that
  each sum node modifies the overall data enough that the independence test will judge an
  independent variable from all others. The base case $h=1$ is trivial: a size $1$ scope generates
  one leaf with distribution equal to data. The case $h=2$ is more interesting and occurs when all
  variables are dependent and belong to the same cluster.

  \textbf{TODO:} Reword this more formally and unambiguously. Also put this after the variable
  independence test and data clustering sections.
\end{proof}

From~\autoref{alg:learngd} we have learned that \code{LearnGD} can be structured into three parts.
The first is discovering variable independencies and judging whether we should partition $V$ and
create a new product node. The second is, if the first part has failed, we must find possible
clusters from the data we have. From these newly discovered clusters, we decide if we should create
another sum node and assign each of its children a partition of these clusters, or if these
clusters all form a single big all encompassing cluster. If this is the case, we create a new
product node whose children are the fully factorized form of the present data. Finally, the third
and last part is the base case. If the scope is of a single variable, we return the univariate
probability distribution given by the univariate data.

If we were to visualize our dataset as a table where rows are instances and columns are variables,
we could equate the algorithm as splitting, either horizontally or vertically, the dataset
according to our partitioning decisions. For instance, if we had decided that there was a certain
subset of variables that were independent of the rest of the variables, we would ``split'' our
dataset table vertically, with each subtable belonging to a variable subset. Similarly for cluster
partitioning, we would split our dataset table horizontally. \autoref{fig:splitting} illustrates
the procedures for variable splitting (\autoref{fig:split-v}) and instance splitting
(\autoref{fig:split-h}).

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[scale=0.2]{imgs/split-v.png}
    \caption{~}\label{fig:split-v}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[scale=0.2]{imgs/split-h.png}
    \caption{~}\label{fig:split-h}
  \end{subfigure}
  \captionsetup{singlelinecheck=false,justification=justified,margin=0cm}
  \caption{These two images represent a dataset in table form. Rows are instances and columns are
  variables.\autoref{fig:split-v} shows variable splitting. In this example, we have observed that
  the subsets $\set{V}_1=\{A,B,C\}$, $\set{V}_2=\{D,X\}$ and $\set{V}_3=\{Y,Z\}$ are independent of
  each other. That is, for every pair $(P,Q), P\in \set{V}_i, Q\in \set{V}_j, i\neq j$, $P$ is
  independent of variable $Q$.  Given these partitions, we create a new product node whose children
  are the recursive calls to \code{LearnGD}. Note that their new scopes are now $V_i$, and their
  data instance covers only their new scope. In this example, the new partitions form subtables
  whose columns are adjacent to each other (e.g.\ $A$, $B$ and $C$ are adjacent). However, we could
  find an independent subset that did not necessarily obey a graphical rule, that is, we could have
  found that $A$ and $Y$ belong to the same partition. In this case, we would have considered $A$
  and $Y$ as a new subset, regardless of their graphical positions. For~\autoref{fig:split-h}, we
  apply a similar concept.  In this case we are clustering similar instances. Note that instance
  partitioning does not alter their scope. Each instance subset $\set{D}_1=\{1,2\}$,
  $\set{D}_2=\{3,4\}$ and $\set{D}_3=\{5,6,7\}$ equates to a discovered cluster. A new sum node is
  then created, with weights corresponding to the ratio of rows in each subtable. The subtables are
  then added as children of the sum node and then recursed. Just like with variable splitting,
  these partitions do not necessarily obey a graphical rule. We could have non-adjacent rows as a
single partition.}\label{fig:splitting} \end{figure}

Now that we have the general idea of the algorithm, we shall describe and analyze how to do both
variable and instance splitting. We will reserve a section to each of these topics. Once we have
covered them both, we shall once again take a broader look at the \code{LearnGD} schema and work on
some other results that depend on the two next sections.

\section{Variable Independence}

The core of variable splitting is finding independence between variables. What we wish to find is
partitions of the current SPN scope such that every element in a partition is independent of all
other elements in other partitions. In this section we shall explain the general idea, describe and
analyze the method used in our implementation and lastly discuss certain problems encountered
during experiments and implementation.

The description of our problem is: given a dataset with a set of variables $\mathbf{V}$, we wish to
find a set $\set{P}=\{\set{P}_1,\set{P}_2,\ldots,\set{P}_n\}$, where $\set{P}_i\cap
\set{P}_j=\emptyset, i\neq j$ and $\set{P}_1\cup \ldots\cup \set{P}_n=\mathbf{V}$ and $\set{P}_i$
is a subset of variables. That is, a set of partitions of $\mathbf{V}$. Additionally, for every
$\set{P}_i$ and $\set{P}_j$, $i\neq j$, $\forall u\in \set{P}_i, v\in \set{P}_j$, $u\perp v$ ($u$
is independent of $v$), where $\perp$ is the independence operator.

Suppose we have an independence oracle $\Omega$ that tells us if $X\perp Y$ and does so in constant
time. A naÃ¯ve solution to this problem is, for every pair $X,Y \in \mathbf{V}$, ask $\Omega$ if
$X\perp Y$. We then memorize which ones are dependent and which are independent. For such
memoization, we can use an undirected graph.

\begin{definition}[Independence Graph]~\\
  Let $G=(\set{V},\set{E})$ be an undirected graph with vertex set $\set{V}$ and edge set
  $\set{E}$. Let $i$ and $j$ be vertices in $\set{V}$. There exists an edge $e_{ij}$ iff
  $i\not\perp j$.
\end{definition}

This reduces our problem to one of finding connected subgraphs. Since there exists an edge if and
only if the two connected variables are dependent, to find a partition $\set{P}_i$, it suffices to
find a connected subgraph in which all of its vertices have no path to another subgraph (there is
no dependence path between variables).

\begin{proposition}
  If $H=(\set{V}',\set{E'})$ is a connected subgraph in Independence Graph $G=(\set{V},\set{E})$
  then $\set{V}'\in\set{P}$.
\end{proposition}
\begin{proof}
  Since there cannot be two same variables in our graph, it suffices to show that, $\forall u'\in
  \set{V}', u\in \set{V}\setminus\set{V}'$: $u'\perp u$. Let us assume that $\set{V}'\not\in
  \set{P}$. That is, there exists an element in $\set{V}'$ that is dependent of an element in
  $\set{V}\setminus\set{V}'$. That means there exists a vertex $v'\in\set{V}'$ and another vertex
  $v\in\set{V}\setminus\set{V}'$ such that an edge $e_{v'v}$ connects both of them. But $u\in
  \set{V}\setminus\set{V}'$ and $\set{V}'$ is the vertex set of a connected subgraph, which means
  such an edge cannot exist, as there is no path from a vertex of $H$ to a vertex in $\set{V}
  \setminus\set{V}'$. Therefore our assumption that $\set{V}'\not\in\set{P}$ is false.
\end{proof}

Sketching our naÃ¯ve solution, we have:

\begin{algorithm}[H]
  \caption{\code{IndepGraph}}\label{alg:indepgraph}
  \begin{algorithmic}[1]
    \Require\,Set $\mathbf{D}$ of instances (data)
    \Require\,Set $\mathbf{V}$ of variables (scope)
    \Ensure\,A set $\mathbf{P}$ of independent partitions of variables
    \State\,Let $\Omega$ be an independence oracle that returns true if independent and false if
      dependent
    \State\,Let $G=(\set{V},\set{E}=\emptyset)$ be the independence graph
    \For{each variable $X\in\set{V}$}\label{alg:line:ig-main-begin}
      \For{each variable $Y\neq X, Y\in\set{V}$}
        \If{$\Omega(X,Y)$ = \textbf{false}}
          \State\,Let $e_{XY}$ be a new edge connecting $X$ and $Y$
          \State\,$\set{E}\gets\set{E}\cup e_{XY}$
        \EndIf%
      \EndFor%
    \EndFor\label{alg:line:ig-main-end}
    \State\,$\mathbf{P}\gets$\texttt{FindConnectedSubgraphs\,(G)}
    \State\,\textbf{return} $\mathbf{P}$
  \end{algorithmic}
\end{algorithm}

We can find the connected subgraphs using a Union-Find structure. Since Union-Find is out of the
scope of this paper, we shall not go into a deep discussion of it. However, we shall assume we have
a Union-Find implementation which uses both \textit{union by rank} and \textit{path compression}
heuristics, bringing the complexity of a series of $m$ Union-Find operations to have amortized time
$\bigo(m\log_2^*(n))$, where $n$ is the number of elements and $\log^*$ is the iterated logarithm
function.

\begin{algorithm}[H]
  \caption{\code{FindConnectedSubgraphs}}\label{alg:findconnectedsubgraphs}
  \begin{algorithmic}[1]
    \Require\,Graph $G=(\set{V},\set{E})$, where $\set{V}$ is vertex set and $\set{E}$ is edge set
    \Ensure\,A set $\set{S}$ of the sets of vertices of the connected subgraphs of $G$
    \State\,Let $\set{U}$ be the set of Union-Find structures.
    \For{each variable $X\in\set{V}$}\label{alg:line:fcs-init-begin}
      \State\,Let $u$ be a new Union-Find structure whose representative is $X$.
      \State\,$u\gets$ \code{MakeSet}\,($X$)
      \State\,$\set{U}\gets\set{U}\cup u$
    \EndFor\label{alg:line:fcs-init-end}
    \For{each variable $X\in\set{V}$}\label{alg:line:fcs-main-begin}
      \For{each variable $Y\in\set{V}$}
        \State\,\mcode{$u_X\gets$ Find}\,($X$)
        \State\,\mcode{$u_Y\gets$ Find}\,($Y$)
        \If{$u_X\neq u_Y$} \code{Union}\,($u_X$, $u_Y$) \EndIf%
      \EndFor%
    \EndFor\label{alg:line:fcs-main-end}
    \State\,Let \code{Convert}\,($\cdot$) be function that converts Union-Finds to set of sets.
    \State\,\textbf{return} \code{Convert}\,($\set{U}$)
  \end{algorithmic}
\end{algorithm}

Let $n=|\set{V}|$, the number of variables. The number of elements in all Union-Finds is
$n$, as we can from lines~\ref{alg:line:fcs-init-begin}--\ref{alg:line:fcs-init-end}. We also know
from lines~\ref{alg:line:fcs-main-begin}--\ref{alg:line:fcs-main-end} that the number of Union-Find
operations we call is ${(2n)}^2+n(n-1)=5n^2-n$. Substituting these values into the amortized
complexity of our Union-Find implementation gives
\begin{equation*}
  (5n^2-n)\log_2^*n.
\end{equation*}
Since we are always differentiating edges $X-Y$ and $Y-X$, we can slightly improve performance by
only taking into account one of those edges. We can decrease our number of Union-Find operations to
$\binom{n}{2}$, which is the number of combinations if we choose $2$ in $n$. The final complexity
comes down to
\begin{equation*}
  \binom{n}{2}\log_2^*n=\left(\frac{n!}{(n-2)!2!}\right)\log_2^*n=\left(\frac{n(n-1)}{2}\right)
    \log_2^*n
\end{equation*}
Let $\alpha(n)=\log_2^*n$. We know that $\alpha(n)$ grows extremely slow, therefore we will assume
$\alpha(n)$ as a constant, giving the final amortized complexity of
\autoref{alg:findconnectedsubgraphs} the asymptotic form of $\bigo((n^2/2-n/2)\log_2^*n)=
\bigo(n^2\alpha(n)-n\alpha(n))=\bigo(n^2)$.

Before we analyze \code{IndepGraph}, we need to review our previous assumption that a certain
independence oracle $\Omega$ returns, in $\bigo(1)$, whether two variables are independent of each
other given data. We will now show two independence tests we used for our implementation. The first
is the standard Chi-Square independence test and the second is the G-test.

%--------------------------------------------------------------------------------------------------

\newpage
\appendix

\newpage

\printbibliography[]

\end{document}
