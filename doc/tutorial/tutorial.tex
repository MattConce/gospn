\documentclass{amsart}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thmtools,thm-restate}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[singlelinecheck=false]{caption}
\usepackage[backend=biber,url=true,doi=true,eprint=false,style=alphabetic]{biblatex}
\usepackage{enumitem}
\usepackage[justification=centering]{caption}
\usepackage{indentfirst}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}

\addbibresource{references.bib}

\makeatletter
\def\subsection{\@startsection{subsection}{3}%
  \z@{.5\linespacing\@plus.7\linespacing}{.1\linespacing}%
  {\normalfont\itshape}}
\makeatother

\makeatletter
\patchcmd{\@setauthors}{\MakeUppercase}{}{}{}
\makeatother

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\algrenewcommand\algorithmicrequire{\textbf{Input}}
\algrenewcommand\algorithmicensure{\textbf{Output}}

\captionsetup[table]{labelsep=space}

\theoremstyle{plain}

\newcounter{dummy-def}\numberwithin{dummy-def}{section}
\newtheorem{definition}[dummy-def]{Definition}
\newcounter{dummy-thm}\numberwithin{dummy-thm}{section}
\newtheorem{theorem}[dummy-thm]{Theorem}
\newcounter{dummy-prop}\numberwithin{dummy-prop}{section}
\newtheorem{proposition}[dummy-prop]{Proposition}
\newcounter{dummy-ex}\numberwithin{dummy-ex}{section}
\newtheorem{exercise}[dummy-ex]{Exercise}
\newcounter{dummy-eg}\numberwithin{dummy-eg}{section}
\newtheorem{example}[dummy-eg]{Example}

\numberwithin{equation}{section}

\newcommand{\set}[1]{\mathbf{#1}}
\newcommand{\pr}{\mathbb{P}}
\renewcommand{\implies}{\Rightarrow}

\newcommand{\bigo}{\mathcal{O}}

\setlength{\parskip}{1em}

\lstset{frameround=fttt,
	numbers=left,
	breaklines=true,
	keywordstyle=\bfseries,
	basicstyle=\ttfamily,
}

\newcommand{\code}[1]{\lstinline[mathescape=true]{#1}}
\newcommand{\mcode}[1]{\lstinline[mathescape]!#1!}


\title{%
  \noindent\rule{13cm}{1.0pt}\\
  \vspace{0.2cm}
  An Introduction to Sum-Product Networks
  \noindent\rule{13cm}{0.8pt}
}
\xdef\shorttitle{Intro to SPNs}
\author[]{\normalsize\textbf{Renato Lui Geh}\\\small Computer Science\\Institute of Mathematics
  and Statistics\\University of SÃ£o Paulo\\\texttt{renatolg@ime.usp.br}}

\begin{document}

\begin{abstract}
  Sum-Product Networks (SPNs) are deep probabilistic graphical models (PGMs) that compactly
  represent tractable probability distributions. Exact inference in SPNs is computed in time linear
  in the number of edges, an attractive feature that distinguishes SPNs from other PGMs. However,
  learning SPNs is a tough task. There have been many advances in learning both the structure and
  parameters of SPNs in the past few years. One interesting feature is the fact that we can make
  use of SPN's deep architecture and perform deep learning on these models. Since the number of
  hidden layers not only does not negatively impact the tractability of inference of SPNs but also
  augments the representability of this model, it is very much desirable to continue research on
  deep learning of SPNs. In this article we seek to produce a tutorial on Sum-Product Networks in
  a simpler, clearer way then how it is currently written in literature. We will introduce SPNs
  and explain how knowledge is represented in this model, how to perform exact inference and
  describe and analyse in detail a simple structural learning algorithm.
  \vspace*{-3.5em}
\end{abstract}

\maketitle

\section{Introduction}

Conventional probabilistic graphical models (PGMs) can compactly represent complex probability
distributions and perform sub-exponential time inference through approximate methods. They are able
to learn from data accurately and have very expressive semantics. However, exact inference in the
general case is intractable. The alternative to exact inference is through the use of approximation
algorithms. Unfortunately, approximate inference is at times unpredictable and analysis of these
algorithms is very difficult.

Sum-Product Networks (SPNs) are deep PGMs that are able to compactly represent tractable
probability distributions. Inference in SPNs is computed in time linear in the number of edges of
the graph, where the number of edges is at most polynomial in the number of variables of the
distribution. SPNs are represented by a DAG where internal nodes are either sum or product nodes.
Leaf nodes are univariate distributions, though recent work on SPNs have shown that multivariate
distributions are also allowed as leaves~\cite{id-spn}. Learning of SPNs can be achieved through
subsequent clusterings of both variables and instanciations, where sum nodes can be seen as
mixtures of distributions and product nodes as variable independencies. An interesting feature of
SPNs is its deep architecture. As shown in Delalleau and Bengio's work~\cite{shallow-vs-deep},
deep SPNs have more representative power than shallow SPNs. Poon and Domingos, on the innaugural
SPN article~\cite{poon-domingos}, were able to learn accurate deep SPNs with 36 layers, as opposed
to the few, less than ten layers that is typically learned in other deep models.

In this article we will provide a comprehensive description of Sum-Product Networks, from its graph
representation and how to perform exact polynomial time inference, to describing and analysing our
implementation of the structural learning algorithm introduced in~\cite{gens-domingos}, a learning
algorithm that is able to learn SPNs of potentially tens of layers.

\section{Sum-Product Networks}



%--------------------------------------------------------------------------------------------------

\newpage
\appendix

\newpage

\printbibliography[]

\end{document}
