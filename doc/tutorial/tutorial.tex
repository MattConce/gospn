\documentclass{amsart}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thmtools,thm-restate}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[singlelinecheck=false]{caption}
\usepackage[backend=biber,url=true,doi=true,eprint=false,style=alphabetic]{biblatex}
\usepackage{enumitem}
\usepackage[justification=centering]{caption}
\usepackage{indentfirst}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}

\addbibresource{references.bib}

\makeatletter
\def\subsection{\@startsection{subsection}{3}%
  \z@{.5\linespacing\@plus.7\linespacing}{.1\linespacing}%
  {\normalfont\itshape}}
\makeatother

\makeatletter
\patchcmd{\@setauthors}{\MakeUppercase}{}{}{}
\makeatother

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\algrenewcommand\algorithmicrequire{\textbf{Input}}
\algrenewcommand\algorithmicensure{\textbf{Output}}

\captionsetup[table]{labelsep=space}

\theoremstyle{plain}

\newcounter{dummy-def}\numberwithin{dummy-def}{section}
\newtheorem{definition}[dummy-def]{Definition}
\newcounter{dummy-thm}\numberwithin{dummy-thm}{section}
\newtheorem{theorem}[dummy-thm]{Theorem}
\newcounter{dummy-prop}\numberwithin{dummy-prop}{section}
\newtheorem{proposition}[dummy-prop]{Proposition}
\newcounter{dummy-ex}\numberwithin{dummy-ex}{section}
\newtheorem{exercise}[dummy-ex]{Exercise}
\newcounter{dummy-eg}\numberwithin{dummy-eg}{section}
\newtheorem{example}[dummy-eg]{Example}

\numberwithin{equation}{section}

\newcommand{\set}[1]{\mathbf{#1}}
\newcommand{\pr}{\mathbb{P}}
\renewcommand{\implies}{\Rightarrow}

\newcommand{\bigo}{\mathcal{O}}

\setlength{\parskip}{1em}

\lstset{frameround=fttt,
	numbers=left,
	breaklines=true,
	keywordstyle=\bfseries,
	basicstyle=\ttfamily,
}

\newcommand{\code}[1]{\lstinline[mathescape=true]{#1}}
\newcommand{\mcode}[1]{\lstinline[mathescape]!#1!}


\title{%
  \noindent\rule{13cm}{1.0pt}\\
  \vspace{0.2cm}
  An Introduction to Sum-Product Networks
  \noindent\rule{13cm}{0.8pt}
}
\xdef\shorttitle{An Introduction to Sum-Product Networks}
\author[]{\normalsize\textbf{Renato Lui Geh}\\\small Computer Science\\Institute of Mathematics
  and Statistics\\University of SÃ£o Paulo\\\texttt{renatolg@ime.usp.br}}

\begin{document}

\begin{abstract}
  Sum-Product Networks (SPNs) are deep probabilistic graphical models (PGMs) that compactly
  represent tractable probability distributions. Exact inference in SPNs is computed in time linear
  in the number of edges, an attractive feature that distinguishes SPNs from other PGMs. However,
  learning SPNs is a tough task. There have been many advances in learning both the structure and
  parameters of SPNs in the past few years. One interesting feature is the fact that we can make
  use of SPN's deep architecture and perform deep learning on these models. Since the number of
  hidden layers not only does not negatively impact the tractability of inference of SPNs but also
  augments the representability of this model, it is very much desirable to continue research on
  deep learning of SPNs. In this article we seek to produce a tutorial on Sum-Product Networks in
  a simpler, clearer way then how it is currently written in literature. We will introduce SPNs
  and explain how knowledge is represented in this model, how to perform exact inference and
  describe and analyse in detail a simple structural learning algorithm.
  \vspace*{-3.5em}
\end{abstract}

\maketitle

\section{Introduction}

Conventional probabilistic graphical models (PGMs) can compactly represent complex probability
distributions and perform sub-exponential time inference through approximate methods. They are able
to learn from data accurately and have very expressive semantics. However, exact inference in the
general case is intractable. The alternative to exact inference is through the use of approximation
algorithms. Unfortunately, approximate inference is at times unpredictable and analysis of these
algorithms is very difficult.

Sum-Product Networks (SPNs) are deep PGMs that are able to compactly represent tractable
probability distributions. Inference in SPNs is computed in time linear in the number of edges of
the graph, where the number of edges is at most polynomial in the number of variables of the
distribution. SPNs are represented by a DAG where internal nodes are either sum or product nodes.
Leaf nodes are univariate distributions, though recent work on SPNs have shown that multivariate
distributions are also allowed as leaves~\cite{id-spn}. Learning of SPNs can be achieved through
subsequent clusterings of both variables and instanciations, where sum nodes can be seen as
mixtures of distributions and product nodes as variable independencies. An interesting feature of
SPNs is its deep architecture. As shown in Delalleau and Bengio's work~\cite{shallow-vs-deep},
deep SPNs have more representative power than shallow SPNs. Poon and Domingos, on the innaugural
SPN article~\cite{poon-domingos}, were able to learn accurate deep SPNs with 36 layers, as opposed
to the few, less than ten layers that is typically learned in other deep models.

In this article we will provide a comprehensive description of Sum-Product Networks, from its graph
representation and how to perform exact polynomial time inference, to describing and analysing our
implementation of the structural learning algorithm introduced in~\cite{gens-domingos}, a learning
algorithm that is able to learn SPNs of potentially tens of layers.

\section{Sum-Product Networks}

A Sum-Product Network is a DAG where nodes can either be sums, products or leaves. In this section
we will present two definitions of SPNs. The first one can be considered ``low level'', whilst the
second carries heavier semantics due to properties we shall enumerate in this section. These
classifications of ``low level'' vs ``high level'' will become clearer as we progress through this
section.

\subsection{Network Polynomials}

Before we define SPNs more formally, we need to understand network polynomials. First introduced by
Darwiche~\cite{diff-approach-darwiche}, a network polynomial is a multilinear function over the
variables of an unnormalized distribution. Computing the marginals of the distribution through its
network polynomial is possible by setting all indicator variables to values consistent with
evidence. The partial derivatives of the network polynomial can be seen as conditioning the network
polynomial to a given event.

Network polynomials are not that attractive by themselves, as the number of terms in the polynomial
is exponential in the general case. Arithmetic circuits (ACs) are a way of representing network
polynomials with a polynomial number of terms. However, ACs are just another way of compiling a
Bayesian network into a polynomial-sized model for tractable distributions.

\begin{definition}[Network polynomial] Let $\mathcal{N}$ be a Bayesian network over set of
  variables $\mathbf{X}$. Let $Pa(X)$ be the set of parents of variable $X$. The network polynomial
  of $\mathcal{N}$ is given by
  \begin{equation*}
    f=\sum_{\mathbf{x}} \prod_{X, Pa(X) \sim \mathbf{x}} \lambda_X \theta_{X|Pa(X)}
  \end{equation*}
  where $\mathbf{x}$ is the set of possible instantiations of $\mathbf{X}$ and $X \sim x$ means
  that $X$ is consistent with instantiation $x$, that is, $\lambda_X = 1$ if $X=x$ and $\lambda_X =
  0$ if $X\neq x$. If the set of evidence does not contain $x$, then all $\lambda_X$ are $1$.
\end{definition}

%--------------------------------------------------------------------------------------------------

\newpage
\appendix

\newpage

\printbibliography[]

\end{document}
